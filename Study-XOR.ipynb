{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib\n",
    "from torch.autograd import Variable\n",
    "matplotlib.use('TkAgg')    # My buggy OSX 10.13.6 requires this\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "N = 2000\n",
    "batch_size = 10\n",
    "epochs = N//batch_size\n",
    "hidden_size = 5\n",
    "output_size = 1\n",
    "lr = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_xor(N):\n",
    "    tmp_x = []\n",
    "    tmp_y = []\n",
    "    for i in range(N):\n",
    "        a = (random.randint(0, 1) == 1)\n",
    "        b = (random.randint(0, 1) == 1)\n",
    "        if (a and not b) or (not a and b):\n",
    "            q = True\n",
    "        else:\n",
    "            q = False\n",
    "        input_features = (a, b)\n",
    "        output_class = q\n",
    "        tmp_x.append(input_features)\n",
    "        tmp_y.append(output_class)\n",
    "    return tmp_x, tmp_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 1])\n"
     ]
    }
   ],
   "source": [
    "# Training set\n",
    "x, y = return_xor(N)\n",
    "x = torch.tensor(x, dtype=torch.float, requires_grad=True)\n",
    "y = torch.tensor(y, dtype=torch.float, requires_grad=True).view(-1 ,1)\n",
    "# Test dataset\n",
    "x_test, y_test = return_xor(100)\n",
    "x_test = torch.tensor(x_test, dtype=torch.float)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \"\"\"Define my own `Dataset` in order to use `Variable` with `autograd`\"\"\"\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(x,y)\n",
    "test_dataset = MyDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 2])\n",
      "torch.Size([2000, 1])\n"
     ]
    }
   ],
   "source": [
    "print(dataset.x.shape)\n",
    "print(dataset.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make data iterable by loading to a loader. Shuffle, batch_size kwargs put them here in order to remind I myself\n",
    "train_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They are 200 batches in the dataset\n",
      "torch.Size([10, 2]) torch.float32\n",
      "torch.Size([10, 1]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(f\"They are {len(train_loader)} batches in the dataset\")\n",
    "shown = 0\n",
    "for (x, y) in train_loader:\n",
    "    if shown == 1:\n",
    "        break\n",
    "    print(f\"{x.shape} {x.dtype}\")\n",
    "    print(f\"{y.shape} {y.dtype}\")\n",
    "    shown += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Binary classification\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, output_size)\n",
    "        # Apply He initialization to first layer because of ReLU\n",
    "        torch.nn.init.normal_(self.fc1.weight, mean=0, std=1) # sqrt(2/2) is 1 See the Fawaz lecture P27\n",
    "        # Apply Xavier to second layer because of Sigmoid\n",
    "        torch.nn.init.xavier_normal_(self.fc2.weight)\n",
    "        \n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    def forward(self, out):\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create my network\n",
    "net = MyModel(dataset.x.shape[1], hidden_size, output_size)\n",
    "\n",
    "# # https://courses.cs.washington.edu/courses/cse446/18wi/sections/section8/XOR-Pytorch.html\n",
    "# def weights_init(model):\n",
    "#     for m in model.modules():\n",
    "#         if isinstance(m, nn.Linear):\n",
    "#             # initialize the weight tensor, here we use a normal distribution\n",
    "#             m.weight.data.normal_(0, 1)\n",
    "\n",
    "# weights_init(net)\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "if CUDA:\n",
    "    net = net.cuda()\n",
    "# In Cross Entropy Loss: Input shape (N,C) and labels shape(N)\n",
    "# In Binary Cross Entropy: Input and output should have the same shape\n",
    "# size_average = True ->losses are averaged over observations for each minibatch -\n",
    "# Got warning use `reduction='elementwise_mean'`\n",
    "# criterion = torch.nn.BCELoss(reduction='elementwise_mean')\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Epoch 1/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 0.2293422967195511,\n",
      "                Training Accuracy: 60\n",
      "              \n",
      "\n",
      "                Epoch 1/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 0.02658955380320549,\n",
      "                Training Accuracy: 82\n",
      "              \n",
      "\n",
      "                Epoch 2/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 0.0024163220077753067,\n",
      "                Training Accuracy: 91\n",
      "              \n",
      "\n",
      "                Epoch 2/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 0.0007453538128174841,\n",
      "                Training Accuracy: 94\n",
      "              \n",
      "\n",
      "                Epoch 3/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 0.0004876851453445852,\n",
      "                Training Accuracy: 95\n",
      "              \n",
      "\n",
      "                Epoch 3/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 0.00037203452666290104,\n",
      "                Training Accuracy: 96\n",
      "              \n",
      "\n",
      "                Epoch 4/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 0.00021987811487633735,\n",
      "                Training Accuracy: 97\n",
      "              \n",
      "\n",
      "                Epoch 4/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 0.00014073603961151093,\n",
      "                Training Accuracy: 97\n",
      "              \n",
      "\n",
      "                Epoch 5/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 0.00011377155169611797,\n",
      "                Training Accuracy: 97\n",
      "              \n",
      "\n",
      "                Epoch 5/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 9.976538422051817e-05,\n",
      "                Training Accuracy: 98\n",
      "              \n",
      "\n",
      "                Epoch 6/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 7.532850577263162e-05,\n",
      "                Training Accuracy: 98\n",
      "              \n",
      "\n",
      "                Epoch 6/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 6.848873454146087e-05,\n",
      "                Training Accuracy: 98\n",
      "              \n",
      "\n",
      "                Epoch 7/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 5.294592847349122e-05,\n",
      "                Training Accuracy: 98\n",
      "              \n",
      "\n",
      "                Epoch 7/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 4.45550904260017e-05,\n",
      "                Training Accuracy: 98\n",
      "              \n",
      "\n",
      "                Epoch 8/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 4.1144099668599665e-05,\n",
      "                Training Accuracy: 98\n",
      "              \n",
      "\n",
      "                Epoch 8/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 3.400981586310081e-05,\n",
      "                Training Accuracy: 98\n",
      "              \n",
      "\n",
      "                Epoch 9/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 3.3032320061465725e-05,\n",
      "                Training Accuracy: 98\n",
      "              \n",
      "\n",
      "                Epoch 9/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.6440602596267127e-05,\n",
      "                Training Accuracy: 98\n",
      "              \n",
      "\n",
      "                Epoch 10/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.201342795160599e-05,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 10/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.039880200754851e-05,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 11/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.787715700629633e-05,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 11/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.818285272747744e-05,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 12/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.4401207408809569e-05,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 12/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.4199925317370798e-05,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 13/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.2443845662346575e-05,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 13/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.289263764192583e-05,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 14/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.1804969290096778e-05,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 14/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.1146109500259627e-05,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 15/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.0126590495929122e-05,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 15/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 8.216735295718536e-06,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 16/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 8.004985829757061e-06,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 16/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 7.740426553937141e-06,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 17/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 7.776385245961137e-06,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 17/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 6.210504125192529e-06,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 18/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 6.244047199288616e-06,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 18/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 6.04722526986734e-06,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 19/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 5.169020823814208e-06,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 19/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 4.66868732473813e-06,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 20/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 4.451242602954153e-06,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 20/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 4.494713266467443e-06,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 21/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 4.020997948828153e-06,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 21/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 3.7324421100493055e-06,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 22/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 3.645529432105832e-06,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 22/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 3.613023864090792e-06,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 23/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 3.211104285583133e-06,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 23/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 3.064443490075064e-06,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 24/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.5422696126042865e-06,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 24/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.5611009277781704e-06,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 25/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.5296501462435117e-06,\n",
      "                Training Accuracy: 99\n",
      "              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Epoch 25/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.270136292281677e-06,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 26/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.153865580112324e-06,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 26/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.8180945744461496e-06,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 27/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.861565806393628e-06,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 27/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.74948877429415e-06,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 28/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.4574181932403008e-06,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 28/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.5117734619707335e-06,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 29/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.4620461570302723e-06,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 29/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.3597843917523278e-06,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 30/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.325650941907952e-06,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 30/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.2425654176695389e-06,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 31/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.153568405243277e-06,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 31/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.108689730244805e-06,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 32/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.0566195669525769e-06,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 32/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 9.971175813916489e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 33/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 9.476424338572542e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 33/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 8.875384764905903e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 34/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 7.435137376887724e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 34/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 6.8554334120563e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 35/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 7.702282118771109e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 35/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 6.985313234508794e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 36/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 7.211741035462182e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 36/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 5.781683398708992e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 37/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 5.402852707447892e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 37/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 5.204973945183156e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 38/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 4.98955103012122e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 38/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 5.025852374274109e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 39/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 4.846720003115479e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 39/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 4.316742092669301e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 40/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 4.249905600772763e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 40/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 4.198654153242387e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 41/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 3.8400543189709424e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 41/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 3.5163802181159554e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 42/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 3.3071410143747926e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 42/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.95690597340581e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 43/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.917855397299718e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 43/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.848758811069274e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 44/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.8767630055881455e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 44/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.3161356921264087e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 45/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.4553696675866377e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 45/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.1406553685210383e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 46/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.1714144793349988e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 46/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.972713192799347e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 47/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.1096406044307514e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 47/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.7868129020826018e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 48/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.7614124203646497e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 48/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.6611883779660275e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 49/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.5587987434173556e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Epoch 49/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.4840958328932174e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 50/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.299706866575434e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 50/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.15538654199554e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 51/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.27070734379231e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 51/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.1551122724995366e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 52/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 8.73665939593593e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 52/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.038056751667682e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 53/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.0089619451036924e-07,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 53/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 8.585815436390476e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 54/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 9.455369820443593e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 54/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 8.626212633089381e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 55/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 7.147934866225114e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 55/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 7.854663408579654e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 56/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 7.290583425856312e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 56/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 6.706558508540184e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 57/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 6.097986471331751e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 57/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 6.391892526380616e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 58/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 5.18215585998405e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 58/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 6.073425140584732e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 59/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 4.911293061127253e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 59/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 4.376519768811704e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 60/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 4.900429573240217e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 60/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 4.5857571961960275e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 61/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 4.2491954843626445e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 61/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 4.026562550052404e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 62/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 3.2399217531065005e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 62/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 3.57100233827623e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 63/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 3.4384562752620695e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 63/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 3.024501182835593e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 64/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 3.267788883931644e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 64/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 3.200813125658897e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 65/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.5547487680910308e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 65/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.316030212057285e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 66/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.336597049179545e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 66/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.545391630803806e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 67/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.1988171283737756e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 67/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.9060472311593912e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 68/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.8839241278101326e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 68/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.8205239982194144e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 69/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.8222086950459015e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 69/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.6436722205526166e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 70/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.560772666664434e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 70/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.503961755133787e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 71/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.524949055919933e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 71/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.4768157363675982e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 72/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.3040487800708433e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 72/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.1641223984781846e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 73/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.0252532156584948e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Epoch 73/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.0747703171887224e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 74/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.0683188556015466e-08,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 74/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 9.276759804777157e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 75/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 9.240950227251687e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 75/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 8.964281761336679e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 76/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 9.219346175370902e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 76/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 8.539744911217895e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 77/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 7.2807444517764e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 77/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 7.549807001794306e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 78/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 7.455938977329879e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 78/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 6.923086992571825e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 79/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 6.144694975063203e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 79/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 5.776462419504469e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 80/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 5.5654507669089526e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 80/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 4.502255457339288e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 81/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 5.006468573043321e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 81/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 4.1039265319398055e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 82/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 4.904804118410766e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 82/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 3.9228340575903076e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 83/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 4.2566208335870215e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 83/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 3.95993771107328e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 84/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 3.677041782168544e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 84/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 3.6428968730461975e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 85/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 3.3556679657209543e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 85/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 3.183876939871766e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 86/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.9989855043766056e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 86/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.9933768797008042e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 87/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.4060482495258384e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 87/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.3970403439932397e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 88/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.81529488610488e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 88/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.1060169164144327e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 89/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.787571868305804e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 89/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.1513266723616198e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 90/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.8015697822448828e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 90/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.6911670952524105e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 91/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.6966177351918077e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 91/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.6265272462234748e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 92/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.6289637416733171e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 92/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.3036809409783245e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 93/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.4220710164991601e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 93/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.3522969410928454e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 94/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.278728900544479e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 94/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.3252821062792464e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 95/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.0381877579845877e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 95/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 9.50294509749483e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 96/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.0436810304881305e-09,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 96/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 9.586774707415202e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 97/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 8.804881268531517e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 97/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 8.445175669891114e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 98/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 8.749989066636488e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Epoch 98/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 8.334430923184755e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 99/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 7.138887259117155e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 99/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 6.899052995557042e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 100/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 5.995646201739646e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 100/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 6.518962036849985e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 101/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 6.122597984159484e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 101/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 6.027792709417668e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 102/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 4.698488265120204e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 102/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 5.485311649344737e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 103/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 4.5933029602096553e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 103/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 4.634329586750141e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 104/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 3.8782291267303037e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 104/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 4.3010453532055237e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 105/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 3.882554000522731e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 105/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 4.063605008042259e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 106/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 3.6260258684528424e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 106/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 3.3087632633765907e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 107/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 3.191974184968416e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 107/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 3.058645836606644e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 108/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.6153013088503485e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 108/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.646020347274458e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 109/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.7586954942648845e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 109/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.1933059535239607e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 110/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.1575295716669274e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 110/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.0807289224933356e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 111/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.189882580827529e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 111/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.1154775153853222e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 112/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.1560930818509405e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 112/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.7165796561968705e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 113/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.591920345545006e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 113/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.562301815694056e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 114/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.54943724517409e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 114/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.6764604982011377e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 115/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.5691357935221362e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 115/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.3559058043011163e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 116/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.3273383780987302e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 116/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.2442226415831925e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 117/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.1426480595044808e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 117/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.1258053517204658e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 118/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 9.673809669985545e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 118/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.0840865011241263e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 119/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 9.860687960605574e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 119/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.0617472179230703e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 120/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.026500343059844e-10,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 120/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 8.58113580193276e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 121/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 7.953650238423648e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 121/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 7.306215354940804e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 122/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 7.05935795930479e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 122/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 7.09579756064116e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Epoch 123/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 6.959118697968947e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 123/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 6.86150164463939e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 124/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 5.622808815775038e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 124/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 6.670856922408319e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 125/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 6.394702822820619e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 125/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 6.208349806469116e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 126/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 5.431675248468437e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 126/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 4.741486370196668e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 127/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 5.1581801330247146e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 127/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 4.1787857896213865e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 128/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 4.543905529619252e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 128/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 3.95411307863025e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 129/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 4.279282483921065e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 129/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 3.777332613363882e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 130/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 3.5755280680671575e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 130/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 3.727891953464457e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 131/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 3.312864219062739e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 131/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 3.3537967542018876e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 132/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 3.5227955968997193e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 132/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 3.051699171141564e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 133/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.723547776195545e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 133/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 3.251778013524387e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 134/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.7270168762028035e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 134/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.45098073781147e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 135/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.625235619169164e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 135/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.393804425515622e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 136/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.0127437910799628e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 136/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.4265959033820117e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 137/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.173835417229597e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 137/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.1411386552649958e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 138/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.9030634706984628e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 138/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.6414298739664268e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 139/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.9425201031042505e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 139/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.9317995120227138e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 140/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.7373781233120944e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 140/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.6203184627361367e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 141/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.5018817786649485e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 141/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.4476969970034226e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 142/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.3956814870486056e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 142/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.6437269947933153e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 143/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.4823062916002883e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 143/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.3956467925790861e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 144/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.3151332453609221e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 144/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.2512985439472324e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 145/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.2660129021513367e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 145/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.1460498348936365e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 146/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.2611051959654507e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 146/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.0778132726596557e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Epoch 147/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.1817239894962306e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 147/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.0872385457216804e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 148/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 9.469504173853505e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 148/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.0544967676828776e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 149/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 1.1002741252819082e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 149/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.0119708890310442e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 150/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 9.969700412448823e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 150/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.0079157994336008e-11,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 151/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 8.30884926833475e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 151/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 7.270273359033519e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 152/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 8.387368057027889e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 152/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 7.94609256865586e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 153/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 8.36327708475526e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 153/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 7.789563265248045e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 154/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 8.183265697014885e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 154/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 7.317356789937612e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 155/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 7.585060184112091e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 155/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 7.912110203123213e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 156/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 7.461311082868072e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 156/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 6.4798661228437915e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 157/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 6.9651363669709365e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 157/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 6.263232554121201e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 158/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 6.406319918994541e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 158/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 6.122923348894638e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 159/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 6.601298500885644e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 159/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 6.62169494583531e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 160/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 5.99553002730846e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 160/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 5.135890844554236e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 161/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 5.7919649978921406e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 161/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 5.185202527763222e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 162/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 5.2877788953020755e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 162/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 5.555039067628442e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 163/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 5.208185879096439e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 163/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 5.324816108875918e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 164/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 4.924147027629555e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 164/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 4.751618803283675e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 165/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 4.907674527182548e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 165/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 4.824733060987407e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 166/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 4.812044859803244e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 166/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 4.647402688379154e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 167/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 4.83719314603448e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 167/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 4.882359707497619e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 168/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 4.0820727874313345e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 168/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 4.361689828757953e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 169/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 4.177976107438974e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 169/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 4.170194137925742e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 170/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 4.226910921972804e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 170/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 4.0103419716996935e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 171/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 4.06103753056164e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 171/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 4.146344292216275e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Epoch 172/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 3.823160971833106e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 172/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 4.333873537820665e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 173/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 3.802233267818922e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 173/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 3.5845501913933653e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 174/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 3.714197352455706e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 174/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 3.826405338414052e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 175/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 3.967547644589953e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 175/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 3.5709592834803905e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 176/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 3.670013945522577e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 176/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 3.5707825585262754e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 177/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 3.2397745510642784e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 177/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 3.2661102555148203e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 178/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 3.5624400564898684e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 178/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 3.2930702435762793e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 179/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 3.358464114450177e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 179/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 3.165781439079529e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 180/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 3.0693253928043562e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 180/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.933613855310435e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 181/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.9286241400722224e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 181/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.9595644511493102e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 182/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.841600435857239e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 182/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.892193862874537e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 183/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 3.169750486392564e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 183/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.956898181166734e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 184/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.765779359009679e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 184/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.854613464012279e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 185/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.885162594945534e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 185/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.893863751060599e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 186/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.9907077248730873e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 186/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.78558101064752e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 187/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.61645024222934e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 187/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.6226863562850422e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 188/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.5130351358898517e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 188/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.505837768188024e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 189/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.741593193786307e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 189/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.907285523434666e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 190/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.4103974025080355e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 190/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.6127129973407826e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 191/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.6233071704490074e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 191/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.4788073069853533e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 192/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.4193249400367156e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 192/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.4066870458333556e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 193/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.4808570996126544e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 193/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.4649689841366174e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 194/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.271027983724583e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 194/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.2987577553285066e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 195/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.4115340800656693e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 195/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.1283684450285056e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                Epoch 196/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.3264456767285724e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 196/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.09447151462705e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 197/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.266273757198234e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 197/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.3543465354353144e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 198/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.3182668892202107e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 198/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.5005093481911267e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 199/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.3516904569531594e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 199/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 2.292734795419915e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 200/200, \n",
      "                Iteration 1/200, \n",
      "                Training Loss: 2.1397701319147977e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n",
      "\n",
      "                Epoch 200/200, \n",
      "                Iteration 101/200, \n",
      "                Training Loss: 1.9344285288186436e-12,\n",
      "                Training Accuracy: 99\n",
      "              \n"
     ]
    }
   ],
   "source": [
    "# Train the network\n",
    "correct_train = 0\n",
    "total_train = 0\n",
    "for epoch in range(epochs):\n",
    "    for i, (batches, labels) in enumerate(train_loader):\n",
    "        batcesh = Variable(batches.float())\n",
    "        labels = Variable(labels.float())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = net(batches)                                      # Forward pass\n",
    "\n",
    "        loss = criterion(output, labels.view(batch_size, 1))            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (output.round().long() == labels.long()).sum()\n",
    "        if i % 100 == 0:\n",
    "            print(f\"\"\"\n",
    "                Epoch {epoch+1}/{epochs}, \n",
    "                Iteration {i+1}/{len(dataset)//batch_size}, \n",
    "                Training Loss: {loss.item()},\n",
    "                Training Accuracy: {100*correct_train/total_train}\n",
    "              \"\"\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0 True\n",
      "0.0 0.0 True\n",
      "0.0 0.0 True\n",
      "1.0 1.0 True\n",
      "1.0 1.0 True\n",
      "1.0 1.0 True\n",
      "0.0 0.0 True\n",
      "0.0 0.0 True\n",
      "1.0 1.0 True\n",
      "0.0 0.0 True\n",
      "0.0 0.0 True\n",
      "1.0 1.0 True\n",
      "0.0 0.0 True\n",
      "0.0 0.0 True\n",
      "0.0 0.0 True\n",
      "1.0 1.0 True\n",
      "1.0 1.0 True\n",
      "0.0 0.0 True\n",
      "0.0 0.0 True\n",
      "1.0 1.0 True\n",
      "1.0 1.0 True\n",
      "1.0 1.0 True\n",
      "1.0 1.0 True\n",
      "1.0 1.0 True\n",
      "1.0 1.0 True\n",
      "0.0 0.0 True\n",
      "1.0 1.0 True\n",
      "1.0 1.0 True\n",
      "1.0 1.0 True\n",
      "0.0 0.0 True\n",
      "1.0 1.0 True\n",
      "1.0 1.0 True\n",
      "0.0 0.0 True\n",
      "1.0 1.0 True\n",
      "1.0 1.0 True\n",
      "1.0 1.0 True\n",
      "0.0 0.0 True\n",
      "1.0 1.0 True\n",
      "1.0 1.0 True\n",
      "1.0 1.0 True\n",
      "1.0 1.0 True\n",
      "1.0 1.0 True\n",
      "1.0 1.0 True\n",
      "1.0 1.0 True\n",
      "1.0 1.0 True\n",
      "1.0 1.0 True\n",
      "1.0 1.0 True\n",
      "1.0 1.0 True\n",
      "1.0 1.0 True\n",
      "1.0 1.0 True\n",
      "0.0 0.0 True\n",
      "0.0 0.0 True\n",
      "0.0 0.0 True\n",
      "0.0 0.0 True\n",
      "0.0 0.0 True\n",
      "1.0 1.0 True\n",
      "1.0 1.0 True\n",
      "1.0 1.0 True\n",
      "0.0 0.0 True\n",
      "1.0 1.0 True\n",
      "1.0 1.0 True\n",
      "0.0 0.0 True\n",
      "0.0 0.0 True\n",
      "1.0 1.0 True\n",
      "1.0 1.0 True\n",
      "0.0 0.0 True\n",
      "0.0 0.0 True\n",
      "0.0 0.0 True\n",
      "0.0 0.0 True\n",
      "0.0 0.0 True\n",
      "1.0 1.0 True\n",
      "0.0 0.0 True\n",
      "1.0 1.0 True\n",
      "0.0 0.0 True\n",
      "1.0 1.0 True\n",
      "0.0 0.0 True\n",
      "0.0 0.0 True\n",
      "1.0 1.0 True\n",
      "1.0 1.0 True\n",
      "0.0 0.0 True\n",
      "0.0 0.0 True\n",
      "0.0 0.0 True\n",
      "1.0 1.0 True\n",
      "0.0 0.0 True\n",
      "1.0 1.0 True\n",
      "0.0 0.0 True\n",
      "1.0 1.0 True\n",
      "0.0 0.0 True\n",
      "1.0 1.0 True\n",
      "0.0 0.0 True\n",
      "1.0 1.0 True\n",
      "1.0 1.0 True\n",
      "0.0 0.0 True\n",
      "1.0 1.0 True\n",
      "0.0 0.0 True\n",
      "0.0 0.0 True\n",
      "1.0 1.0 True\n",
      "0.0 0.0 True\n",
      "1.0 1.0 True\n",
      "1.0 1.0 True\n",
      "Testing Accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Test the network\n",
    "# WIP\n",
    "correct_answer = 0\n",
    "for i, (batches, labels) in enumerate(test_dataset):\n",
    "    batches = Variable(batches.float())\n",
    "    labels = Variable(labels.float())\n",
    "\n",
    "    output = net(batches)                       \n",
    "    assertion = output.round().item() == labels.item()\n",
    "    print(output.round().item(), labels.item(), assertion)\n",
    "    correct_answer += assertion\n",
    "print(f\"Testing Accuracy: {100 * correct_answer/test_dataset.x.shape[0]}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
